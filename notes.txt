Initial embedding: Use SBERT embeddings (paraphrase-multilingual-mpnet-base-v2).
---
from sklearn.metrics import silhouette_score

silhouette_tf_idf = silhouette_score(umap_tf_idf, cluster_labels_tf_idf)
silhouette_sbert = silhouette_score(umap_sbert, cluster_labels_sbert)

from sklearn.metrics import davies_bouldin_score

db_index_tf_idf = davies_bouldin_score(umap_tf_idf, cluster_labels_tf_idf)
db_index_sbert = davies_bouldin_score(umap_sbert, cluster_labels_sbert)
!!!!
 Qualitative / Visual evaluation (key for interpretability):
Visual clarity of groups: Which embedding approach provides more intuitive, well-separated clusters on the UMAP 2D plot?
Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
Human-in-the-loop check:
Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.

1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
3. Evaluate and Compare:
Visual inspection: Check which method produces clearer, visually meaningful clusters.
Cluster evaluation metrics:
Silhouette Score: Measures how well separated clusters are.
Davies-Bouldin Index: Evaluates cluster compactness and separation.
Semantic/interpretability evaluation: Check how easy it is to interpret clusters.

----

To achieve explainability comparable to TF-IDF:

Use SBERT embeddings to cluster semantically meaningful groups.
Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.

-----
t-sne vs umap?
---------
also draw and compare extracted labels
---------
lematize?

---------
tune tfidf

-------
create clusters on embeddings not on umap projections



You're totally right to think this through carefully â€” if URL topic labels are meaningful, they should definitely group articles better than random. So getting an average NPMI of -0.0776 for URL topics might seem surprisingly low.

Letâ€™s dig deeper into why thatâ€™s happening, and how you can confirm that the labels are still better than random, even if clusters are better.

âœ… What could cause low NPMI on meaningful labels?

ðŸ“‰ 1. Small group sizes or highly uneven label distribution
NPMI depends on having enough co-occurrence signal.
If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
âœ… Fix:

Skip small groups:

if len(group_texts) < 5:
    continue
ðŸŒ€ 2. High lexical variety within URL topics
Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

Cover broad subtopics (e.g. politics, labor, society),
Use varied vocab (diluting co-occurrence),
Contain common words, reducing informativeness.
So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

ðŸ¤– 3. Tokenization, stopword handling, and preprocessing
If you're using raw word counts with minimal preprocessing:

NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).
âœ… Fixes:

Use lemmatization (not just tokenization).
Apply custom Slovene stopword list.
Lowercase, remove punctuation, etc.
ðŸ§ª 4. Random still possible to beat bad labels
If the label groups are too noisy or small, then random samples of size ~5 may coincidentally have more co-occurring words (especially common ones).

âœ… Test it: Compare to true random grouping

Letâ€™s prove it â€” create random groups the same size as your URL topic groups and compare NPMI.

âœ¨ Script Snippet:
def generate_random_groups(n_items, group_sizes):
    indices = np.random.permutation(n_items)
    groups = {}
    start = 0
    for i, size in enumerate(group_sizes):
        groups[f"rand_{i}"] = list(indices[start:start+size])
        start += size
    return groups

# Get URL label group sizes
label_group_sizes = [len(v) for v in label_groups.values() if len(v) >= 5]

# Generate random groups with same sizes
random_groups = generate_random_groups(len(texts), label_group_sizes)

# Compute NPMI for random groups
npmi_random = compute_npmi(random_groups, texts, stopwords=slovene_stopwords)

# Print comparison
print(f"Avg NPMI - Random: {np.mean(list(npmi_random.values())):.4f}")
print(f"Avg NPMI - URL Topics: {np.mean(list(npmi_labels.values())):.4f}")
ðŸ” Outcome:

If URL topics still beat random â†’ âœ… Youâ€™re right.
If random beats URL labels â†’ something's wrong with labels, text quality, or preprocessing.
âœ… Summary

Observation	Possible Cause	Fix/Check
URL labels give low NPMI	Small groups, varied content, weak keyword overlap	Filter small groups, lemmatize, clean
Clusters beat URL labels	HDBSCAN found tighter semantic groups	This is OK â€” it shows your pipeline adds value
Random matches URL NPMI	Label noise, sparse keywords	Run comparison with controlled random groups
Would you like me to wrap this random vs. label NPMI test into a reusable function and print all three side-by-side?






Maybe recursive clustering

WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
compare to
umap to 2D -> hbdscan

Test integrity when using random, cluster and knn cluster corr

preprocess for without adjectives