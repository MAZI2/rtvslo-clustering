---
D Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
D Human-in-the-loop check:
    Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.
---------
1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
TODO: Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
----
D To achieve explainability comparable to TF-IDF:
    Use SBERT embeddings to cluster semantically meaningful groups.
    Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.
D t-sne vs umap?
D also draw and compare extracted labels
D lematize?
D tune tfidf
D create clusters on embeddings not on umap projections

D On preprocessed tokens extract keywords
------
TODO: NPMI on true_labels
    âœ… What could cause low NPMI on meaningful labels?

    ğŸ“‰ 1. Small group sizes or highly uneven label distribution
    NPMI depends on having enough co-occurrence signal.
    If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
    âœ… Fix:

   Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

    Cover broad subtopics (e.g. politics, labor, society),
    Use varied vocab (diluting co-occurrence),
    Contain common words, reducing informativeness.
    So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

    NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
    Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).

-------- TODO: should I normalize or to 50D?
WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
TODO: is there upper bound other than time for clusterin ... should you reduce dims at all?

TODO: if embedding-based evaluation is >99% what does it tell you
âœ… Why Use Embedding-Based Evaluation?
Traditional metrics (e.g., Silhouette, Davies-Bouldin) rely on distances between points and fail when:

Clusters are non-spherical
Data is high-dimensional
There's semantic overlap between labels
Using embeddings allows you to evaluate semantic coherence and separation independently of geometry.

âœ… Options for Embedding-Based Evaluation
1. Intra-cluster vs Inter-cluster Cosine Similarity
2. Cluster Centroid Cosine Distances
    âœ… Why It Might Be Better
    Works directly on SBERT or BERT embeddings (semantic!)
    Not distorted by dimensionality reduction (e.g., UMAP or t-SNE)
    Unsupervised â€” you don't need ground-truth labels
    Doesnâ€™t assume spherical geometry like Silhouette
âœ… Alternative: NMI or ARI with Ground Truth Labels
    If you have something like URL-based topics, you can also compute:

TODO: How does NPMI eval work currently. I have ground truth but how should i use embedding similarity for comparing which one is better sbert or tsne

âš–ï¸ Comparison: Word Co-occurrence (PMI/NPMI) vs Embedding Similarity
Aspect	PMI/NPMI (Co-occurrence)	Embedding-based (SBERT etc.)
Goal	Measure if words in cluster co-occur meaningfully	Measure if article meanings are semantically close
Unit	Words	Articles (vectors)
Basis	Word frequency & co-occurrence	Language model embeddings
Type	Symbolic/statistical	Neural/semantic
Pros	- Unsupervised
- Interpretable (keyword-level)
- Captures topical coherence	- Captures deep semantic relationships
- Works well for short texts
- Does not depend on token overlap
Cons	- Sensitive to corpus size
- Doesnâ€™t handle synonyms well
- Ignores meaning beyond word co-occurrence	- Opaque (harder to interpret)
- Can fail for mixed-topic clusters
Output	Coherence scores per cluster (e.g. NPMI)	Similarity stats per cluster (intra/inter)

TODO: why are npmi keywords almost the same and not better?
âœ… When to Use Each
NPMI: For keyword coherence
Cosine similarity: For semantic tightness in embedding space
    Together, they give a well-rounded view:

    Do words co-occur?
    Are texts semantically similar?
    Do generated cluster labels reflect the content?

TODO: pmi/npmi keyword extraction ------
Instead of just picking top TF-IDF words, you rank combinations of words that co-occur more than expected. That way, you get:

Thematically consistent words
Fewer irrelevant or generic words (like â€œdejalaâ€ or â€œlahkoâ€)
Words that tend to appear together in the same context
âœ… Example Use: Gensimâ€™s CoherenceModel
You can do something like:

âœ… Compared to TF-IDF
Feature	TF-IDF	PMI/NPMI
Focus	Individual word frequency	Word pairs and co-occurrence
Pros	Simple, fast	Captures topical coherence
Common artifacts	Words like â€œdejalaâ€, â€œletoâ€, etc.	Fewer generic terms
Interpretability	Medium	High (shows word relationships)
Language dependent?	Somewhat	Less so â€” works on token co-occurrence



{'Embedding': 'SBERT', 'Silhouette': 0.31920457, 'Davies-Bouldin': 1.0093945344303572, 'ARI': 0.2322979492079684, 'NMI': 0.49488931728741375, 'Num Clusters': 26, 'Noise Points': 0, 'Embedding Alignment': 0.9991281, 'Avg NPMI': 0.08708324263406085}
{'Embedding': 'TFIDF', 'Silhouette': 0.32474416, 'Davies-Bouldin': 1.0185792309972688, 'ARI': 0.17818023641450698, 'NMI': 0.42428634930358, 'Num Clusters': 27, 'Noise Points': 0, 'Embedding Alignment': 0.9994208, 'Avg NPMI': 0.09520921248178979}











Ukraine has to help Slovenia and Ukraine has to help Russia

Ilka Stuhec enters fifth best time at world's first world in royal discipline

The government has confirmed the source of the legislation on the ownership of workers .

Los Angeles Kings win series

Slovenia has banned the transport of dead and displaced animals

Divaa is a city in the southwestern part of the country .

Dallas' coaches played the 45th series of finals in the league of basketball , and until now only two games were eliminated from 4:0 .

In the car, we made the first mile with a new damage to the tire, which is technically similar to the superpower, so this time we looked a little more detailed about what the new one brings. It's gonna be a lot of years before electric vehicles are blown off the roads with internal combustion engines. At Volvo, only 100% of electric vehicles were thought to be manufactured. The executive director of Volva Jim Rowan was moved to change.

electric cars are a big part of the electric car industry .

The Four concert will be broadcast live on Slovenian TV on 23 September at 20 a.m.

ğŸ§  Cluster 0 KOÅ ARKA â†’ igra, skok, toÄka, tekma, donÄiÄ‡, podaj, dallas, sezona, minuta, dober, met, Äetrtina, leto, liga, zadnji, igra skok, zmaga, ekipa, trojka, nba
size of cluster: 701
ğŸ§  Cluster 1 HOKEJ â†’ tekma, gol, minuta, kopitar, los, zmaga, angeles, sezona, los angeles, strel, tretjina, edmonton, toÄka, kralj, zadnji, liga, konÄnica, obramba, ekipa, mreÅ¾a
size of cluster: 214
ğŸ§  Cluster 2 TENIS â†’ niz, igra, turnir, zda, finale, leto, Ä‘okoviÄ‡, dvoboj, sinner, rus, dober, jorgiÄ‡, igralec, olimpijski, zmaga, servis, fra, alcaraz, toÄka, polfinale
size of cluster: 522
ğŸ§  Cluster 3 DIRKANJE - TUDI KOLESARJENJE â†’ dirka, etapa, pogaÄar, leto, mesto, ekipa, zmaga, dober, sezona, kolesar, cilj, rogliÄ, zadnji, sekunda, dan, svetoven, konec, velik, Äas, kilometer
size of cluster: 1150
ğŸ§  Cluster 4 IZRAEL, PALESTINAâ†’ izraelski, gaza, izrael, napad, Älovek, palestinski, hamas, vojska, drÅ¾ava, izraelski vojska, libanon, ubit, pomoÄ, leto, sila, obmoÄje, ameriÅ¡ki, vojna, humanitaren, palestinec
size of cluster: 1645
ğŸ§  Cluster 5 SMUÄŒARSKI SKOKI - TUDI OLIMPIJSKE DISCIPLINE â†’ mesto, tekma, dober, olimpijski, svetoven, leto, finale, sezona, igra, pokal, toÄka, slovenski, sekunda, svetoven pokal, medalja, meter, prevc, skok, prvenstvo, Äas
size of cluster: 1865
ğŸ§  Cluster 6 AMERIKA - VLADA â†’ trump, predsednik, zda, leto, ameriÅ¡ki, harris, biden, volitev, drÅ¾ava, trumpov, zvezen, predsedniÅ¡ki, kandidat, nekdanji, sodiÅ¡Äe, donald, donald trump, demokratski, demokrat, sooÄenje
size of cluster: 586
ğŸ§  Cluster 7 NOGOMET â†’ minuta, tekma, toÄka, leto, real, barcelona, Å¾oga, liga, sezona, prvak, gol, klub, strel, manchester, finale, liverpool, zmaga, dober, zadetek, madrid
size of cluster: 720
ğŸ§  Cluster 8 SLOVENSKI Å PORT1 â†’ minuta, tekma, olimpija, toÄka, strel, blok, gol, liga, celje, sezona, zmaga, zadnji, ekipa, dober, Å¾oga, maribor, konec, polÄas, krka, vrata
size of cluster: 1019
ğŸ§  Cluster 9 SLOVENSKI Å PORT2 â†’ tekma, leto, sezona, klub, liga, ekipa, trener, dober, nov, igralec, reprezentanca, prvenstvo, olimpija, slovenski, prvak, evropski, slovenija, igra, pogodba, velik
size of cluster: 699
ğŸ§  Cluster 10 SLOVENSKI Å PORT3 â†’ tekma, slovenija, minuta, slovenski, dober, prvenstvo, toÄka, leto, igra, reprezentanca, skupina, liga, gol, evropski, ekipa, olimpijski, zmaga, zadnji, mesto, igralec
size of cluster: 1407
ğŸ§  Cluster 11 AVTOMOBILI, INDUSTRIJA â†’ elektriÄen, vozilo, avtomobil, leto, nov, model, velik, podjetje, voÅ¾nja, avtomobilski, motor, odstotek, kitajski, slovenija, elektriÄen vozilo, evro, evropski, pogon, proizvajalec, znamka
size of cluster: 546
ğŸ§  Cluster 12 GOSPODARSTVO â†’ odstotek, evro, leto, energija, cena, milijon, slovenija, milijon evro, nov, jedrski, drÅ¾ava, rast, podjetje, velik, elektrarna, zakon, projekt, druÅ¾ba, visok, podneben
size of cluster: 975
ğŸ§  Cluster 13 GASILSTVO, POÅ½ARI â†’ poÅ¾ar, gasilec, Älovek, eksplozija, obmoÄje, gasilski, objekt, poÅ¡kodovan, hiÅ¡a, velik, policija, stavba, ogenj, ura, mesto, Å¡koda, gaÅ¡enje, leto, dom, uprava
size of cluster: 317
ğŸ§  Cluster 14 VREME - TUDI TURIZEM â†’ temperatura, stopinja, sneg, slovenija, leto, dan, padavina, veter, celzij, stopinja celzij, moÄen, drÅ¾ava, velik, poplava, reka, visok, del, obmoÄje, ura, voda
size of cluster: 546
ğŸ§  Cluster 15 TURIZEM â†’ leto, Å¾ival, nov, slovenija, velik, kmetijski, drÅ¾ava, voda, Älovek, kmet, evro, hrana, odstotek, obmoÄje, ministrstvo, dan, mesto, milijon, vrsta, slovenski
size of cluster: 1112
ğŸ§  Cluster 16 NESREÄŒE â†’ nesreÄa, Älovek, letalo, voznik, leten, vozilo, policist, obmoÄje, leto, prometen, prometen nesreÄa, helikopter, kraj, poÅ¡kodovan, policija, reÅ¡evalec, drÅ¾ava, potres, policijski, ura
size of cluster: 939
ğŸ§  Cluster 17 ZAKONODAJA - TUDI GOSPODARSTVO â†’ leto, odstotek, zakon, predlog, vlada, evro, ministrstvo, delo, slovenija, javen, sindikat, plaÄa, nov, podjetje, delavec, deloven, pravica, drÅ¾ava, zaposlen, Å¡ola
size of cluster: 1925
ğŸ§  Cluster 18 RUSIJA, UKRAJINA â†’ ruski, ukrajina, ukrajinski, rusija, napad, drÅ¾ava, slovenija, leto, predsednik, vojna, evropski, vojaÅ¡ki, zda, zelenski, sila, Älovek, oblast, ameriÅ¡ki, trump, vojska
size of cluster: 2044
ğŸ§  Cluster 19 SLAVNI â†’ leto, film, smrt, vloga, skupina, kralj, slovenski, dan, delo, druÅ¾ina, velik, igralec, Å¾ivljenje, britanski, Äas, Älovek, svet, nekdanji, star, glasben
size of cluster: 548
ğŸ§  Cluster 20 SODIÅ ÄŒE, POLICIJA, KRIMINAL â†’ policija, policist, napad, dejanje, moÅ¡ki, Älovek, leto, policijski, kazniv, kazniv dejanje, preiskava, Å¡ola, otrok, osumljenec, leten, voznik, vozilo, ljubljana, mesto, Å¾rtev
size of cluster: 933
ğŸ§  Cluster 21 OBLASTI â†’ stranka, leto, volitev, vlada, sodiÅ¡Äe, predsednik, nov, evropski, drÅ¾ava, slovenija, odstotek, kandidat, komisija, policija, minister, poslanec, svet, parlament, drÅ¾aven, zakon
size of cluster: 2741
ğŸ§  Cluster 22 UMETNOST, KULTURA â†’ leto, razstava, muzej, delo, slovenski, nov, umetnost, mesto, del, slovenija, svet, umetnik, velik, galerija, prostor, Älovek, projekt, kulturen, Äas, kultura
size of cluster: 2587
ğŸ§  Cluster 23 GLASBA â†’ leto, festival, album, koncert, glasba, glasben, pesem, skladba, nov, skupina, zasedba, slovenski, velik, dan, slovenija, oder, dober, svet, Älovek, mesto
size of cluster: 1502
ğŸ§  Cluster 24 FILM â†’ film, leto, predstava, filmski, festival, nagrada, slovenski, zgodba, dober, gledaliÅ¡Äe, nov, reÅ¾iser, serija, vloga, Å¾ivljenje, Älovek, svet, velik, Äas, Å¾enska
size of cluster: 1384
ğŸ§  Cluster 25 LITERATURA â†’ leto, knjiga, roman, nagrada, slovenski, pesem, zgodba, zbirka, Å¾ivljenje, delo, Älovek, svet, literaren, poezija, Äas, pisatelj, jezik, knjiÅ¾nica, beseda, nov
size of cluster: 866

Initial embedding: Use SBERT embeddings (paraphrase-multilingual-mpnet-base-v2).
---
from sklearn.metrics import silhouette_score

silhouette_tf_idf = silhouette_score(umap_tf_idf, cluster_labels_tf_idf)
silhouette_sbert = silhouette_score(umap_sbert, cluster_labels_sbert)

from sklearn.metrics import davies_bouldin_score

db_index_tf_idf = davies_bouldin_score(umap_tf_idf, cluster_labels_tf_idf)
db_index_sbert = davies_bouldin_score(umap_sbert, cluster_labels_sbert)
!!!!
 Qualitative / Visual evaluation (key for interpretability):
Visual clarity of groups: Which embedding approach provides more intuitive, well-separated clusters on the UMAP 2D plot?
Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
Human-in-the-loop check:
Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.

1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
3. Evaluate and Compare:
Visual inspection: Check which method produces clearer, visually meaningful clusters.
Cluster evaluation metrics:
Silhouette Score: Measures how well separated clusters are.
Davies-Bouldin Index: Evaluates cluster compactness and separation.
Semantic/interpretability evaluation: Check how easy it is to interpret clusters.

----

To achieve explainability comparable to TF-IDF:

Use SBERT embeddings to cluster semantically meaningful groups.
Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.

-----
t-sne vs umap?
---------
also draw and compare extracted labels
---------
lematize?

---------
tune tfidf

-------
create clusters on embeddings not on umap projections



You're totally right to think this through carefully â€” if URL topic labels are meaningful, they should definitely group articles better than random. So getting an average NPMI of -0.0776 for URL topics might seem surprisingly low.

Letâ€™s dig deeper into why thatâ€™s happening, and how you can confirm that the labels are still better than random, even if clusters are better.

âœ… What could cause low NPMI on meaningful labels?

ğŸ“‰ 1. Small group sizes or highly uneven label distribution
NPMI depends on having enough co-occurrence signal.
If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
âœ… Fix:

Skip small groups:

if len(group_texts) < 5:
    continue
ğŸŒ€ 2. High lexical variety within URL topics
Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

Cover broad subtopics (e.g. politics, labor, society),
Use varied vocab (diluting co-occurrence),
Contain common words, reducing informativeness.
So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

ğŸ¤– 3. Tokenization, stopword handling, and preprocessing
If you're using raw word counts with minimal preprocessing:

NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).
âœ… Fixes:

Use lemmatization (not just tokenization).
Apply custom Slovene stopword list.
Lowercase, remove punctuation, etc.
ğŸ§ª 4. Random still possible to beat bad labels
If the label groups are too noisy or small, then random samples of size ~5 may coincidentally have more co-occurring words (especially common ones).

âœ… Test it: Compare to true random grouping

Letâ€™s prove it â€” create random groups the same size as your URL topic groups and compare NPMI.

âœ¨ Script Snippet:
def generate_random_groups(n_items, group_sizes):
    indices = np.random.permutation(n_items)
    groups = {}
    start = 0
    for i, size in enumerate(group_sizes):
        groups[f"rand_{i}"] = list(indices[start:start+size])
        start += size
    return groups

# Get URL label group sizes
label_group_sizes = [len(v) for v in label_groups.values() if len(v) >= 5]

# Generate random groups with same sizes
random_groups = generate_random_groups(len(texts), label_group_sizes)

# Compute NPMI for random groups
npmi_random = compute_npmi(random_groups, texts, stopwords=slovene_stopwords)

# Print comparison
print(f"Avg NPMI - Random: {np.mean(list(npmi_random.values())):.4f}")
print(f"Avg NPMI - URL Topics: {np.mean(list(npmi_labels.values())):.4f}")
ğŸ” Outcome:

If URL topics still beat random â†’ âœ… Youâ€™re right.
If random beats URL labels â†’ something's wrong with labels, text quality, or preprocessing.
âœ… Summary

Observation	Possible Cause	Fix/Check
URL labels give low NPMI	Small groups, varied content, weak keyword overlap	Filter small groups, lemmatize, clean
Clusters beat URL labels	HDBSCAN found tighter semantic groups	This is OK â€” it shows your pipeline adds value
Random matches URL NPMI	Label noise, sparse keywords	Run comparison with controlled random groups
Would you like me to wrap this random vs. label NPMI test into a reusable function and print all three side-by-side?






Maybe recursive clustering

WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
compare to
umap to 2D -> hbdscan

Test integrity when using random, cluster and knn cluster corr

preprocess for without adjectives








ntic alignment using embeddings, and it can give you a more direct, model-based evaluation of clustering quality â€” especially when traditional clustering metrics fail (like silhouette score on 2D projections).

âœ… Why Use Embedding-Based Evaluation?
Traditional metrics (e.g., Silhouette, Davies-Bouldin) rely on distances between points and fail when:

Clusters are non-spherical
Data is high-dimensional
There's semantic overlap between labels
Using embeddings allows you to evaluate semantic coherence and separation independently of geometry.

âœ… Options for Embedding-Based Evaluation
1. Intra-cluster vs Inter-cluster Cosine Similarity

Measure how similar articles in a cluster are to each other (intra)
Compare to similarity between articles in different clusters (inter)
The higher the gap, the better
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def average_intra_inter_similarity(embeddings, labels):
    unique_clusters = np.unique(labels[labels >= 0])
    intra_sims = []
    inter_sims = []

    for cluster in unique_clusters:
        idx = np.where(labels == cluster)[0]
        cluster_embeds = embeddings[idx]
        if len(cluster_embeds) < 2:
            continue

        sim_matrix = cosine_similarity(cluster_embeds)
        intra_sims.append(np.mean(sim_matrix[np.triu_indices_from(sim_matrix, k=1)]))

        # Compare to a few random other clusters
        other_idx = np.where(labels != cluster)[0]
        sampled_other = np.random.choice(other_idx, size=min(len(idx), len(other_idx)), replace=False)
        inter_sim = cosine_similarity(cluster_embeds, embeddings[sampled_other])
        inter_sims.append(np.mean(inter_sim))

    return np.mean(intra_sims), np.mean(inter_sims)
2. Cluster Centroid Cosine Distances

Compute each cluster's centroid
Measure:
How close points are to their centroid (cohesion)
How far centroids are from each other (separation)
âœ… Why It Might Be Better
Works directly on SBERT or BERT embeddings (semantic!)
Not distorted by dimensionality reduction (e.g., UMAP or t-SNE)
Unsupervised â€” you don't need ground-truth labels
Doesnâ€™t assume spherical geometry like Silhouette
âœ… Alternative: NMI or ARI with Ground Truth Labels
If you have something like URL-based topics, you can also compute:

from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

nmi = normalized_mutual_info_score(true_labels, predicted_clusters)
ari = adjusted_rand_score(true_labels, predicted_clusters)
But embeddings give you a more model-based semantic check.

Would you like me to help implement this for your current SBERT setup?

You said:
i think that you once suggested the word coocurrance or something like that to measure semantics. How does that compare
ChatGPT said:
Yes, youâ€™re right â€” earlier I suggested using word co-occurrence-based metrics, like PMI/NPMI, to evaluate the semantic coherence of clusters. Letâ€™s compare the two approaches in depth:

âš–ï¸ Comparison: Word Co-occurrence (PMI/NPMI) vs Embedding Similarity
Aspect	PMI/NPMI (Co-occurrence)	Embedding-based (SBERT etc.)
Goal	Measure if words in cluster co-occur meaningfully	Measure if article meanings are semantically close
Unit	Words	Articles (vectors)
Basis	Word frequency & co-occurrence	Language model embeddings
Type	Symbolic/statistical	Neural/semantic
Pros	- Unsupervised
- Interpretable (keyword-level)
- Captures topical coherence	- Captures deep semantic relationships
- Works well for short texts
- Does not depend on token overlap
Cons	- Sensitive to corpus size
- Doesnâ€™t handle synonyms well
- Ignores meaning beyond word co-occurrence	- Opaque (harder to interpret)
- Can fail for mixed-topic clusters
Output	Coherence scores per cluster (e.g. NPMI)	Similarity stats per cluster (intra/inter)
âœ… When to Use Each
Use PMI/NPMI if:

You care about word-level coherence
You want to interpret what a cluster is about
Youâ€™re working with many short docs (titles, leads)
Use Embedding-based similarity if:

You want to test semantic tightness
Your documents are longer or richer (e.g., paragraphs)
Youâ€™re clustering based on embeddings (SBERT etc.)
ğŸ§  Best Practice: Use Both
Many NLP papers evaluate topic clusters using both:

NPMI: For keyword coherence
Cosine similarity: For semantic tightness in embedding space
Together, they give a well-rounded view:

Do words co-occur?
Are texts semantically similar?
Do generated cluster labels reflect the content?
Would you like a script that outputs both NPMI and embedding coherence for your clusters?

You said:
so can you use pmi/npmi to extract meaningful words for cluster?
ChatGPT said:
Yes â€” you can absolutely use PMI/NPMI to extract meaningful keywords for a cluster. In fact, this is one of the original purposes of those metrics in topic modeling!

âœ… How PMI/NPMI Helps for Keyword Extraction
Instead of just picking top TF-IDF words, you rank combinations of words that co-occur more than expected. That way, you get:

Thematically consistent words
Fewer irrelevant or generic words (like â€œdejalaâ€ or â€œlahkoâ€)
Words that tend to appear together in the same context
âœ… Example Use: Gensimâ€™s CoherenceModel
You can do something like:

from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

def extract_coherent_keywords(token_lists, top_n=10):
    dictionary = Dictionary(token_lists)
    corpus = [dictionary.doc2bow(text) for text in token_lists]

    cm = CoherenceModel(
        texts=token_lists,
        corpus=corpus,
        dictionary=dictionary,
        coherence='c_npmi',
        topn=top_n
    )

    # This will rank words by their NPMI-based coherence
    return cm.get_coherence_per_topic()
This is typically used after clustering documents, where each cluster is treated like a "topic".

TODO: SloBERTa
16 clustrov
TODO: tf-idf, svd na 15(silihouette), kmeans, t-sne v 2d

âœ… Compared to TF-IDF
Feature	TF-IDF	PMI/NPMI
Focus	Individual word frequency	Word pairs and co-occurrence
Pros	Simple, fast	Captures topical coherence
Common artifacts	Words like â€œdejalaâ€, â€œletoâ€, etc.	Fewer generic terms
Interpretability	Medium	High (shows word relationships)
Language dependent?	Somewhat	Less so â€” works on token co-occurrence
ğŸ§  Best Strategy
Use Stanza-preprocessed token lists (e.g. NOUN, PROPN, ADJ only)
For each cluster, treat the collection of docs as a â€œtopicâ€
Run CoherenceModel with coherence='c_npmi' to extract top words
Would you like a full working example that does this per cluster?

You said:
for explainability is there any reason other than speed, to use methods other than llm or bert embeddings and summarization for clusters, if i have whole articles embedded and can also embed only leads if i would need to to reduce the load
ChatGPT said:
Thatâ€™s a great and forward-thinking question â€” and the short answer is:

If you're already using LLMs or BERT-style embeddings + summarization, youâ€™re near the best you can do in terms of explainability â€” especially if you can afford the compute.
But here's the nuanced breakdown:
âœ… When LLMs/BERT Embeddings & Summarization Win

They are ideal when you want:

Semantic depth: They capture context, tone, and meaning
Natural summaries: You can generate human-readable cluster labels
Consistency: They behave similarly across documents
Multilingual support: Especially useful for Slovene
Explainability via sentence: Not just top keywords, but a sentence that can describe a whole group
So when might other methods still make sense?
Case	Alternative Might Be Better?	Why
Compute-limited environments	Yes	TF-IDF is fast & light
Need transparent scoring	Yes	TF-IDF & NPMI are explicitly ranked
Keyword interpretability is preferred	Sometimes	You might want to show top-N words, not just a sentence
Auditability / reproducibility	Yes	Easier to explain to non-technical users
Short documents (titles only)	Maybe	Keywords might perform just as well as LLMs
Example: Combine the Best of Both
A very explainable pipeline might look like:

Cluster documents (with SBERT or similar)
Extract top NPMI keywords per cluster
Generate sentence summaries using an LLM on those keywords
Optionally: Add a few representative article titles to each cluster
Display all in a UI: sentence + keywords + example headlines
TL;DR
If:

You can run mBART or Flan-T5
You have document-level embeddings
Youâ€™re using keyword-based summaries only as a fallback
Then youâ€™re already in the best explainability zone â€” and donâ€™t need simpler alternatives unless for speed, audience clarity, or audits.

Let me know if you'd like to generate cluster cards or summaries that combine:

Sentence label
Top keywords
Most representative article title(s) (e.g., closest to cluster centroid)

also how well description sentences are suited

























Ukraine has to help Slovenia and Ukraine has to help Russia

Ilka Stuhec enters fifth best time at world's first world in royal discipline

The government has confirmed the source of the legislation on the ownership of workers .

Los Angeles Kings win series

Slovenia has banned the transport of dead and displaced animals

Divaa is a city in the southwestern part of the country .

Dallas' coaches played the 45th series of finals in the league of basketball , and until now only two games were eliminated from 4:0 .

In the car, we made the first mile with a new damage to the tire, which is technically similar to the superpower, so this time we looked a little more detailed about what the new one brings. It's gonna be a lot of years before electric vehicles are blown off the roads with internal combustion engines. At Volvo, only 100% of electric vehicles were thought to be manufactured. The executive director of Volva Jim Rowan was moved to change.

electric cars are a big part of the electric car industry .

The Four concert will be broadcast live on Slovenian TV on 23 September at 20 a.m.



{0: " HOKEJ",
1: " KOÅ ARKA",
2: " AMERIKA - VLADA",
3: " DIRKANJE - TUDI KOLESARJENJE",
4: " TENIS",
5: " IZRAEL, PALESTINA",
6: " SMUÄŒARSKI SKOKI - TUDI OLIMPIJSKE DISCIPLINE",
7: " NOGOMET",
8: " SLOVENSKI Å PORTI - TUDI KOÅ ARKA IN HOKEJ",
9: " SLOVENSKI Å PORTI - TUDI KOÅ ARKA IN HOKEJ",
10: " AVTOMOBILI, LETALA",
11: " GOSPODARSTVO",
12: " GASILSTVO, POÅ½ARI",
13: " TURIZEM",
14: " VREME - TUDI TURIZEM",
15: " SLOVENSKA POLITIKA",
16: " NESREÄŒE",
17: " POMEMBNI LJUDJE",
18: " ZAKONODAJA - TUDI GOSPODARSTVO",
19: " SODIÅ ÄŒE, POLICIJA, KRIMINAL",
20: " OBLASTI",
21: " ZGODOVINA",
22: " UMETNOST",
23: " GLASBA",
24: " LITERATURA",
25: " FILM"}


{0: "KOÅ ARKA",
1: "HOKEJ",
2: "TENIS",
3: "DIRKANJE - TUDI KOLESARJENJE",
4: "IZRAEL, PALESTINA",
5: "SMUÄŒARSKI SKOKI - TUDI OLIMPIJSKE DISCIPLINE",
6: "AMERIKA - VLADA",
7: "NOGOMET",
8: "SLOVENSKI Å PORT1",
9: "SLOVENSKI Å PORT2",
10: "SLOVENSKI Å PORT3",
11: "AVTOMOBILI, INDUSTRIJA",
12: "GOSPODARSTVO",
13: "GASILSTVO, POÅ½ARI",
14: "VREME - TUDI TURIZEM",
15: "TURIZEM",
16: "NESREÄŒE",
17: "ZAKONODAJA - TUDI GOSPODARSTVO",
18: "RUSIJA, UKRAJINA",
19: "SLAVNI",
20: "SODIÅ ÄŒE, POLICIJA, KRIMINAL",
21: "OBLASTI",
22: "UMETNOST, KULTURA",
23: "GLASBA",
24: "FILM",
25: "LITERATURA"}