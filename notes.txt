Initial embedding: Use SBERT embeddings (paraphrase-multilingual-mpnet-base-v2).",
---
from sklearn.metrics import silhouette_score

silhouette_tf_idf = silhouette_score(umap_tf_idf, cluster_labels_tf_idf)
silhouette_sbert = silhouette_score(umap_sbert, cluster_labels_sbert)

from sklearn.metrics import davies_bouldin_score

db_index_tf_idf = davies_bouldin_score(umap_tf_idf, cluster_labels_tf_idf)
db_index_sbert = davies_bouldin_score(umap_sbert, cluster_labels_sbert)
!!!!
 Qualitative / Visual evaluation (key for interpretability):
Visual clarity of groups: Which embedding approach provides more intuitive, well-separated clusters on the UMAP 2D plot?
Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
Human-in-the-loop check:
Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.

1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
3. Evaluate and Compare:
Visual inspection: Check which method produces clearer, visually meaningful clusters.
Cluster evaluation metrics:
Silhouette Score: Measures how well separated clusters are.
Davies-Bouldin Index: Evaluates cluster compactness and separation.
Semantic/interpretability evaluation: Check how easy it is to interpret clusters.

----

To achieve explainability comparable to TF-IDF:

Use SBERT embeddings to cluster semantically meaningful groups.
Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.

-----
D t-sne vs umap?
D also draw and compare extracted labels
D lematize?
D tune tfidf
D create clusters on embeddings not on umap projections
------



You're totally right to think this through carefully â€” if URL topic labels are meaningful, they should definitely group articles better than random. So getting an average NPMI of -0.0776 for URL topics might seem surprisingly low.

Letâ€™s dig deeper into why thatâ€™s happening, and how you can confirm that the labels are still better than random, even if clusters are better.

âœ… What could cause low NPMI on meaningful labels?

ðŸ“‰ 1. Small group sizes or highly uneven label distribution
NPMI depends on having enough co-occurrence signal.
If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
âœ… Fix:

Skip small groups:

if len(group_texts) < 5:
    continue
ðŸŒ€ 2. High lexical variety within URL topics
Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

Cover broad subtopics (e.g. politics, labor, society),
Use varied vocab (diluting co-occurrence),
Contain common words, reducing informativeness.
So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

ðŸ¤– 3. Tokenization, stopword handling, and preprocessing
If you're using raw word counts with minimal preprocessing:

NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).
âœ… Fixes:

Use lemmatization (not just tokenization).
Apply custom Slovene stopword list.
Lowercase, remove punctuation, etc.
ðŸ§ª 4. Random still possible to beat bad labels
If the label groups are too noisy or small, then random samples of size ~5 may coincidentally have more co-occurring words (especially common ones).

âœ… Test it: Compare to true random grouping

Letâ€™s prove it â€” create random groups the same size as your URL topic groups and compare NPMI.

âœ¨ Script Snippet:
def generate_random_groups(n_items, group_sizes):
    indices = np.random.permutation(n_items)
    groups = {}
    start = 0
    for i, size in enumerate(group_sizes):
        groups[f"rand_{i}"] = list(indices[start:start+size])
        start += size
    return groups

# Get URL label group sizes
label_group_sizes = [len(v) for v in label_groups.values() if len(v) >= 5]

# Generate random groups with same sizes
random_groups = generate_random_groups(len(texts), label_group_sizes)

# Compute NPMI for random groups
npmi_random = compute_npmi(random_groups, texts, stopwords=slovene_stopwords)

# Print comparison
print(f"Avg NPMI - Random: {np.mean(list(npmi_random.values())):.4f}")
print(f"Avg NPMI - URL Topics: {np.mean(list(npmi_labels.values())):.4f}")
ðŸ” Outcome:

If URL topics still beat random â†’ âœ… Youâ€™re right.
If random beats URL labels â†’ something's wrong with labels, text quality, or preprocessing.
âœ… Summary

Observation	Possible Cause	Fix/Check
URL labels give low NPMI	Small groups, varied content, weak keyword overlap	Filter small groups, lemmatize, clean
Clusters beat URL labels	HDBSCAN found tighter semantic groups	This is OK â€” it shows your pipeline adds value
Random matches URL NPMI	Label noise, sparse keywords	Run comparison with controlled random groups
Would you like me to wrap this random vs. label NPMI test into a reusable function and print all three side-by-side?






Maybe recursive clustering

WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
compare to
umap to 2D -> hbdscan

Test integrity when using random, cluster and knn cluster corr

preprocess for without adjectives








ntic alignment using embeddings, and it can give you a more direct, model-based evaluation of clustering quality â€” especially when traditional clustering metrics fail (like silhouette score on 2D projections).

âœ… Why Use Embedding-Based Evaluation?
Traditional metrics (e.g., Silhouette, Davies-Bouldin) rely on distances between points and fail when:

Clusters are non-spherical
Data is high-dimensional
There's semantic overlap between labels
Using embeddings allows you to evaluate semantic coherence and separation independently of geometry.

âœ… Options for Embedding-Based Evaluation
1. Intra-cluster vs Inter-cluster Cosine Similarity

Measure how similar articles in a cluster are to each other (intra)
Compare to similarity between articles in different clusters (inter)
The higher the gap, the better
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def average_intra_inter_similarity(embeddings, labels):
    unique_clusters = np.unique(labels[labels >= 0])
    intra_sims = []
    inter_sims = []

    for cluster in unique_clusters:
        idx = np.where(labels == cluster)[0]
        cluster_embeds = embeddings[idx]
        if len(cluster_embeds) < 2:
            continue

        sim_matrix = cosine_similarity(cluster_embeds)
        intra_sims.append(np.mean(sim_matrix[np.triu_indices_from(sim_matrix, k=1)]))

        # Compare to a few random other clusters
        other_idx = np.where(labels != cluster)[0]
        sampled_other = np.random.choice(other_idx, size=min(len(idx), len(other_idx)), replace=False)
        inter_sim = cosine_similarity(cluster_embeds, embeddings[sampled_other])
        inter_sims.append(np.mean(inter_sim))

    return np.mean(intra_sims), np.mean(inter_sims)
2. Cluster Centroid Cosine Distances

Compute each cluster's centroid
Measure:
How close points are to their centroid (cohesion)
How far centroids are from each other (separation)
âœ… Why It Might Be Better
Works directly on SBERT or BERT embeddings (semantic!)
Not distorted by dimensionality reduction (e.g., UMAP or t-SNE)
Unsupervised â€” you don't need ground-truth labels
Doesnâ€™t assume spherical geometry like Silhouette
âœ… Alternative: NMI or ARI with Ground Truth Labels
If you have something like URL-based topics, you can also compute:

from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

nmi = normalized_mutual_info_score(true_labels, predicted_clusters)
ari = adjusted_rand_score(true_labels, predicted_clusters)
But embeddings give you a more model-based semantic check.

Would you like me to help implement this for your current SBERT setup?

You said:
i think that you once suggested the word coocurrance or something like that to measure semantics. How does that compare
ChatGPT said:
Yes, youâ€™re right â€” earlier I suggested using word co-occurrence-based metrics, like PMI/NPMI, to evaluate the semantic coherence of clusters. Letâ€™s compare the two approaches in depth:

âš–ï¸ Comparison: Word Co-occurrence (PMI/NPMI) vs Embedding Similarity
Aspect	PMI/NPMI (Co-occurrence)	Embedding-based (SBERT etc.)
Goal	Measure if words in cluster co-occur meaningfully	Measure if article meanings are semantically close
Unit	Words	Articles (vectors)
Basis	Word frequency & co-occurrence	Language model embeddings
Type	Symbolic/statistical	Neural/semantic
Pros	- Unsupervised
- Interpretable (keyword-level)
- Captures topical coherence	- Captures deep semantic relationships
- Works well for short texts
- Does not depend on token overlap
Cons	- Sensitive to corpus size
- Doesnâ€™t handle synonyms well
- Ignores meaning beyond word co-occurrence	- Opaque (harder to interpret)
- Can fail for mixed-topic clusters
Output	Coherence scores per cluster (e.g. NPMI)	Similarity stats per cluster (intra/inter)
âœ… When to Use Each
Use PMI/NPMI if:

You care about word-level coherence
You want to interpret what a cluster is about
Youâ€™re working with many short docs (titles, leads)
Use Embedding-based similarity if:

You want to test semantic tightness
Your documents are longer or richer (e.g., paragraphs)
Youâ€™re clustering based on embeddings (SBERT etc.)
ðŸ§  Best Practice: Use Both
Many NLP papers evaluate topic clusters using both:

NPMI: For keyword coherence
Cosine similarity: For semantic tightness in embedding space
Together, they give a well-rounded view:

Do words co-occur?
Are texts semantically similar?
Do generated cluster labels reflect the content?
Would you like a script that outputs both NPMI and embedding coherence for your clusters?

You said:
so can you use pmi/npmi to extract meaningful words for cluster?
ChatGPT said:
Yes â€” you can absolutely use PMI/NPMI to extract meaningful keywords for a cluster. In fact, this is one of the original purposes of those metrics in topic modeling!

âœ… How PMI/NPMI Helps for Keyword Extraction
Instead of just picking top TF-IDF words, you rank combinations of words that co-occur more than expected. That way, you get:

Thematically consistent words
Fewer irrelevant or generic words (like â€œdejalaâ€ or â€œlahkoâ€)
Words that tend to appear together in the same context
âœ… Example Use: Gensimâ€™s CoherenceModel
You can do something like:

from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

def extract_coherent_keywords(token_lists, top_n=10):
    dictionary = Dictionary(token_lists)
    corpus = [dictionary.doc2bow(text) for text in token_lists]

    cm = CoherenceModel(
        texts=token_lists,
        corpus=corpus,
        dictionary=dictionary,
        coherence='c_npmi',
        topn=top_n
    )

    # This will rank words by their NPMI-based coherence
    return cm.get_coherence_per_topic()
This is typically used after clustering documents, where each cluster is treated like a "topic".

âœ… Compared to TF-IDF
Feature	TF-IDF	PMI/NPMI
Focus	Individual word frequency	Word pairs and co-occurrence
Pros	Simple, fast	Captures topical coherence
Common artifacts	Words like â€œdejalaâ€, â€œletoâ€, etc.	Fewer generic terms
Interpretability	Medium	High (shows word relationships)
Language dependent?	Somewhat	Less so â€” works on token co-occurrence
ðŸ§  Best Strategy
Use Stanza-preprocessed token lists (e.g. NOUN, PROPN, ADJ only)
For each cluster, treat the collection of docs as a â€œtopicâ€
Run CoherenceModel with coherence='c_npmi' to extract top words
Would you like a full working example that does this per cluster?

You said:
for explainability is there any reason other than speed, to use methods other than llm or bert embeddings and summarization for clusters, if i have whole articles embedded and can also embed only leads if i would need to to reduce the load
ChatGPT said:
Thatâ€™s a great and forward-thinking question â€” and the short answer is:

If you're already using LLMs or BERT-style embeddings + summarization, youâ€™re near the best you can do in terms of explainability â€” especially if you can afford the compute.
But here's the nuanced breakdown:
âœ… When LLMs/BERT Embeddings & Summarization Win

They are ideal when you want:

Semantic depth: They capture context, tone, and meaning
Natural summaries: You can generate human-readable cluster labels
Consistency: They behave similarly across documents
Multilingual support: Especially useful for Slovene
Explainability via sentence: Not just top keywords, but a sentence that can describe a whole group
So when might other methods still make sense?
Case	Alternative Might Be Better?	Why
Compute-limited environments	Yes	TF-IDF is fast & light
Need transparent scoring	Yes	TF-IDF & NPMI are explicitly ranked
Keyword interpretability is preferred	Sometimes	You might want to show top-N words, not just a sentence
Auditability / reproducibility	Yes	Easier to explain to non-technical users
Short documents (titles only)	Maybe	Keywords might perform just as well as LLMs
Example: Combine the Best of Both
A very explainable pipeline might look like:

Cluster documents (with SBERT or similar)
Extract top NPMI keywords per cluster
Generate sentence summaries using an LLM on those keywords
Optionally: Add a few representative article titles to each cluster
Display all in a UI: sentence + keywords + example headlines
TL;DR
If:

You can run mBART or Flan-T5
You have document-level embeddings
Youâ€™re using keyword-based summaries only as a fallback
Then youâ€™re already in the best explainability zone â€” and donâ€™t need simpler alternatives unless for speed, audience clarity, or audits.

Let me know if you'd like to generate cluster cards or summaries that combine:

Sentence label
Top keywords
Most representative article title(s) (e.g., closest to cluster centroid)

also how well description sentences are suited

























Ukraine has to help Slovenia and Ukraine has to help Russia

Ilka Stuhec enters fifth best time at world's first world in royal discipline

The government has confirmed the source of the legislation on the ownership of workers .

Los Angeles Kings win series

Slovenia has banned the transport of dead and displaced animals

Divaa is a city in the southwestern part of the country .

Dallas' coaches played the 45th series of finals in the league of basketball , and until now only two games were eliminated from 4:0 .

In the car, we made the first mile with a new damage to the tire, which is technically similar to the superpower, so this time we looked a little more detailed about what the new one brings. It's gonna be a lot of years before electric vehicles are blown off the roads with internal combustion engines. At Volvo, only 100% of electric vehicles were thought to be manufactured. The executive director of Volva Jim Rowan was moved to change.

electric cars are a big part of the electric car industry .

The Four concert will be broadcast live on Slovenian TV on 23 September at 20 a.m.





ðŸ§  HOKEJ Cluster 0 â†’ tekma, gol, minuta, kopitar, los, zmaga, angeles, sezona, los angeles, strel, 
            tretjina, edmonton, toÄka, kralj, zadnji, liga, konÄnica, obramba, mreÅ¾a, podaljÅ¡k
---- Boston za vodilnimi Rangersi zaostaja samo Å¡e toÄko
---- Daccord prvi, ki je na zimski klasiki ohranil mreÅ¾o nedotaknjeno
---- Kingsi prviÄ letos streljali s praznimi naboji, zmaga Toronta
---- ÄŒetrti zaporedni poraz Kraljev, RdeÄa krila slavila po loteriji
---- Carolina v zadnji tretjini zabila kar pet golov
ðŸ§  KOÅ ARKA Cluster 1 â†’ igra, skok, toÄka, tekma, donÄiÄ‡, podaj, dallas, sezona, minuta, dober, met, Äetrtina, leto, 
                liga, zadnji, igra skok, zmaga, ekipa, trojka, nba
---- Pelikani pokvarili LeBronovo rojstnodnevno slavje
---- Utah se je z izjemno predstavo oddolÅ¾il Dallasu za visok poraz
---- Oklahoma City porazil Å¡e najboljÅ¡o ekipo Vzhoda
---- NajlaÅ¾je dihajo pri Baltimoru in San Franciscu, na voljo Å¡e pet prostih mest
---- DonÄiÄ‡ in Irving Å¾e do odmora potopila odpisani Portland
ðŸ§  AMERIKA - VLADA Cluster 2 â†’ trump, predsednik, zda, leto, ameriÅ¡ki, harris, biden, volitev, drÅ¾ava, trumpov, predsedniÅ¡ki, 
            kandidat, zvezen, nekdanji, donald, demokratski, sodiÅ¡Äe, donald trump, demokrat, sooÄenje
---- Vplivni in bogati v strahu pred objavo 180 imen, povezanih z Epsteinom
---- Trump se je pritoÅ¾il na odloÄitev o izkljuÄitvi iz glasovanja v Mainu
---- Trump s pritoÅ¾bo na vrhovno sodiÅ¡Äe tudi glede prepovedi kandidature v Koloradu
---- AmeriÅ¡ko sodiÅ¡Äe razkrilo imena posameznikov v povezavi z Epsteinom
---- Biden ob tretji obletnici vdora v Kapitol ostro nad Trumpa: OdloÄa se o demokraciji
ðŸ§  DIRKANJE - TUDI KOLESARJENJE Cluster 3 â†’ dirka, etapa, pogaÄar, leto, mesto, ekipa, zmaga, dober, kolesar, sezona, rogliÄ, cilj, zadnji, 
            sekunda, dan, svetoven, konec, velik, Äas, kilometer
---- Barbara JoliÄ: Motivacija sledi akciji
---- RdeÄi biki prevzemajo RogliÄevo novo ekipo
---- Po puÅ¡Äavi in gorah tudi Simon MarÄiÄ in Toni Mulec
---- Prolog za uvod v reli Dakar: MarÄiÄ 76., Mulec 96.
---- MarÄiÄa po teÅ¾avah reÅ¡evali s helikopterjem
ðŸ§  TENIS Cluster 4 â†’ niz, igra, turnir, zda, finale, leto, Ä‘okoviÄ‡, dvoboj, sinner, rus, dober, jorgiÄ‡, igralec, zmaga, 
            fra, servis, olimpijski, alcaraz, toÄka, polfinale
---- Kvitova na porodniÅ¡ki dopust, Osaka se je z njega vrnila
---- Rafael Nadal
---- Nadal zanesljivo ugnal Thiema po 350 dneh premora
---- De Minaur konÄal niz 43 zmag ÄokoviÄ‡a v Avstraliji 
---- Nadal zapravil tri zakljuÄne Å¾oge in ostal brez polfinala
ðŸ§  IZRAEL, PALESTINA Cluster 5 â†’ izraelski, gaza, izrael, napad, Älovek, palestinski, hamas, vojska, drÅ¾ava, izraelski vojska, libanon, 
            ubit, pomoÄ, leto, sila, obmoÄje, ameriÅ¡ki, vojna, humanitaren, palestinec
---- Izrael vse glasneje o selitvi Palestincev iz Gaze. Oblasti skrbi obtoÅ¾ba zaradi genocida.
---- AmeriÅ¡ka letalonosilka Gerald R. Ford zapuÅ¡Äa Sredozemlje
---- V Libanonu ubit namestnik vodje Hamasa. Izrael bo na ICJ-ju izpodbijal obtoÅ¾be genocida.
---- Claudine Gay po obtoÅ¾bah antisemitizma in plagiatorstva odstopila kot predsednica Harvarda
---- Ugledne izraelske javne osebnosti obsodile "hujskanje h genocidu"
ðŸ§  SMUÄŒARSKI SKOKI - TUDI OLIMPIJSKE DISCIPLINE Cluster 6 â†’ mesto, tekma, dober, svetoven, olimpijski, leto, finale, sezona, pokal, toÄka, sekunda, 
        svetoven pokal, slovenski, prevc, meter, skok, medalja, velik, Äas, prvenstvo
---- Po odstopih Urevc in LiÄefa edina slovenska predstavnica ostaja Mandeljc 
---- Chebet v Barceloni postavila nov svetovni rekord, Botolin pa drÅ¾avnega
---- Hrgota: Finalni skok eden najboljÅ¡ih v LaniÅ¡kovi karieri
---- Nika Prevc s petim mestom v Oberstdorfu zmagovalka turneje dveh veÄerov
---- Popoln skok AnÅ¾eta LaniÅ¡ka v leto 2024
ðŸ§  NOGOMET Cluster 7 â†’ minuta, tekma, toÄka, real, barcelona, leto, Å¾oga, liga, sezona, prvak, gol, strel, manchester, 
        klub, liverpool, finale, zmaga, dober, madrid, zadetek
---- Liverpool se je po trilerju na Anfieldu utrdil na vrhu
---- Manchester United van de Beeka posodil v Frankfurt
---- Rooney Å¾e po 83 dneh zapustil klop Birminghama
---- RÃ¼diger potopil Mallorco, Girona pa Oblakov Atletico
---- PSG osvojil francoski superpokal
ðŸ§  Cluster 8 â†’ minuta, tekma, olimpija, toÄka, strel, blok, gol, liga, celje, zmaga, sezona, zadnji, ekipa, dober, 
            Å¾oga, maribor, konec, polÄas, krka, vrata
---- Velika zmaga Olimpije na MadÅ¾arskem
---- JeseniÄani imajo s toÄko v Zell am Seeju usodo v zadnjem krogu v svojih rokah
---- Krka v drugem polÄasu pometla s tekmecem
---- Izjemni Zajc najprej zreÅ¾iral preobrat, nato zabil Å¡e hat-trick
---- Zvezda v devetem poskusu ugnala FenerbahÄe v Carigradu; Barca porazila Real
ðŸ§  SLOVENSKI Å PORTI - TUDI KOÅ ARKA IN HOKEJ Cluster 9 â†’ tekma, slovenija, leto, slovenski, minuta, dober, igra, liga, prvenstvo, sezona, olimpijski, 
        reprezentanca, ekipa, toÄka, klub, evropski, igralec, gol, skupina, mesto
---- "Vseh dobrih stvari je enkrat konec, a sanje o koÅ¡arki bodo vselej z mano"
---- Pianigiani zapustil Cedevito Olimpijo, na ljubljanski klopi znova MartiÄ‡
---- Hapoel iÅ¡Äe zamenjavo za DÅ¾ikiÄ‡a, glavni kandidat Golemac
---- Decembra Luka DonÄiÄ‡ v slogu MVP-ja lige
---- V ZDA in Kanadi zaÅ¾ivela nova profesionalna Å¾enska liga
ðŸ§  AVTOMOBILI, LETALA Cluster 10 â†’ elektriÄen, vozilo, avtomobil, leto, nov, model, velik, voÅ¾nja, avtomobilski, podjetje, motor, 
        odstotek, kitajski, slovenija, elektriÄen vozilo, evro, evropski, pogon, znamka, proizvajalec
---- Kitajska na dobri poti do najveÄjega izvoznika avtomobilov
---- Maserati se s posebnimi izvedenkami poslavlja od osemvaljnikov
---- Avtonomna voÅ¾nja dobila turkizno barvo
---- Wizz Air zaÄasno prekinja letalsko povezavo med Ljubljano in londonskim letaliÅ¡Äem Luton
---- 50 let Volkswagnovega golfa
ðŸ§  GOSPODARSTVO Cluster 11 â†’ odstotek, evro, leto, energija, cena, milijon, slovenija, milijon evro, nov, jedrski, drÅ¾ava, 
            zakon, podjetje, projekt, elektrarna, druÅ¾ba, velik, rast, podneben, vlada
---- PodraÅ¾ila se je cestnina za predor Karavanke
---- Kateri ukrepi za blaÅ¾enje energetske draginje ostajajo v letu 2024?
---- Na HrvaÅ¡kem niÄ veÄ oznaÄevanja cen v kunah, le Å¡e v evrih
---- Gospodarska aktivnost v obmoÄju evra decembra upadla
---- DrÅ¾ava se bo zadolÅ¾ila za 1,5 milijarde evrov
ðŸ§  GASILSTVO, POÅ½ARI Cluster 12 â†’ poÅ¾ar, gasilec, Älovek, eksplozija, obmoÄje, gasilski, poÅ¡kodovan, objekt, hiÅ¡a, policija, velik, 
            stavba, gaÅ¡enje, ura, ogenj, mesto, Å¡koda, leto, dom, kraj
---- PoÅ¾ar v podjetju na Ljubnem ob Savinji pogaÅ¡en, poziv obÄanom preklican
---- PoÅ¾ar v Podjetju na Ljubnem ob Savinji povzroÄil za okoli dva milijona Å¡kode
---- V NemÄiji poÅ¾ar v bolniÅ¡nici terjal Å¾ivljenje, veÄ kot 20 poÅ¡kodovanih
---- Na Tirolskem z balkonov reÅ¡evali goste iz goreÄega hotela
---- Å tevilo Å¾rtev eksplozije tovornjaka cisterne v Liberiji naraslo na 74
ðŸ§  TURIZEM Cluster 13 â†’ leto, Å¾ival, slovenija, nov, velik, odstotek, drÅ¾ava, evro, Älovek, kmetijski, mesto, voda, kmet, dan, 
            milijon, hrana, slovenski, turist, obmoÄje, ministrstvo
---- Turizem v letu 2023 â€“ rekord turistov, boj za leÅ¾alnike in zvezdice za Slovenijo
---- Letalska druÅ¾ba ponujala dvakratno silvestrovanje, a je potnike pustila na cedilu
---- Iz Å anghaja izplula prva na Kitajskem izdelana kriÅ¾arka
---- Slovenija med redkimi evropskimi drÅ¾avami, ki praznujejo tudi 2. januar
---- Hans Habe
ðŸ§  VREME - TUDI TURIZEM Cluster 14 â†’ temperatura, stopinja, sneg, leto, slovenija, dan, padavina, veter, celzij, drÅ¾ava, stopinja celzij, moÄen, velik, poplava, reka, visok, del, obmoÄje, ura, voda
---- Na skrajnem severu Evrope v novo leto vstopili pri minus 40 stopinjah Celzija
---- Letos bomo lahko iz Slovenije opazovali komete, konjunkcijo planetov in severni sij
---- Neurje Henk povzroÄa teÅ¾ave v delih Evrope
---- MoÄno deÅ¾evje in poplave v delih Evrope, na severu Evrope obilno sneÅ¾enje in mraz
---- Kranjskogorski turizem si tudi zaradi Zlate lisice mane roke
ðŸ§  SLOVENSKA POLITIKA Cluster 15 â†’ ruski, ukrajina, ukrajinski, rusija, napad, drÅ¾ava, slovenija, leto, predsednik, evropski, vojna, vojaÅ¡ki, zda, Älovek, oblast, zelenski, sila, ameriÅ¡ki, vlada, vojska
---- Skupina BRICS odslej z novimi polnopravnimi Älanicami
---- Prenehala je obstajati separatistiÄna republika Gorski Karabah
---- Slovenija dve leti na stolÄku nestalne Älanice Varnostnega sveta. Fajon: Å½elimo pustiti svoj peÄat.
---- Zelenski obljublja pospeÅ¡eno izdelavo ukrajinskega oroÅ¾ja, Putin pa napoveduje krepitev napadov
---- Predsednica novoletni obisk namenila starostnikom v DomÅ¾alah
ðŸ§  NESREÄŒE Cluster 16 â†’ nesreÄa, Älovek, letalo, voznik, leten, obmoÄje, vozilo, policist, leto, prometen, prometen nesreÄa, helikopter, kraj, poÅ¡kodovan, reÅ¡evalec, policija, drÅ¾ava, potres, policijski, ura
---- Iz Japonske po potresu poroÄila o Å¾rtvah pod zruÅ¡enimi stavbami
---- Pri Benetkah se je prevrnil avtobus z 79 turisti iz BiH-a
---- Vseh 379 ljudi na potniÅ¡kem letalu ubeÅ¾alo ognju, pet Älanov posadke letala obalne straÅ¾e umrlo
---- Å tevilo smrtnih Å¾rtev letalskih nesreÄ lani med najmanjÅ¡imi v zadnjih desetletjih
---- V obÄini Bohinj plaz odnesel tri alpiniste, med njimi sta dva laÅ¾je poÅ¡kodovana
ðŸ§  POMEMBNI LJUDJE Cluster 17 â†’ leto, film, smrt, vloga, skupina, druÅ¾ina, igralec, dan, slovenski, velik, Å¾ivljenje, delo, kralj, Äas, britanski, rak, nekdanji, svet, star, glasben
---- Od "Å¾urerskega princa" do novega danskega kralja
---- Poslovila se je Francoise Delbart, ovekoveÄena na "simbolu sreÄnega trenutka"
---- Umrl je likovni umetnik NejÄ Slapar, ki se je uvrÅ¡Äal v sam vrh geometrijske abstrakcije
---- Kdo je Mary? Po odstopu Margarete II. svet dobil prvo avstralsko kraljico Å¾eno.
---- Hollywood Å¾aluje za igralko Glynis Johns, znano po vlogi mame sufraÅ¾etke v filmu Mary Poppins
ðŸ§  ZAKONODAJA - TUDI GOSPODARSTVO Cluster 18 â†’ odstotek, leto, zakon, evro, predlog, vlada, delo, slovenija, javen, sindikat, ministrstvo, podjetje, plaÄa, nov, delavec, deloven, zaposlen, Å¡ola, visok, otrok
---- V veljavi podraÅ¾itve napotitev delavcev
---- Na HrvaÅ¡kem sta se ponoÄi rodili dvojÄici, prva v letu 2023, druga v 2024
---- Zakon o dolgotrajni oskrbi za zdaj prinaÅ¡a nove pravice oskrbovalcem druÅ¾inskih Älanov
---- Prvi deÄek se je rodil v Ljubljani, prva deklica v Mariboru
---- Popravek na delniÅ¡kih trgih, bitcoin prviÄ po aprilu 2022 nad mejo 45 tisoÄ
ðŸ§  SODIÅ ÄŒE, POLICIJA, KRIMINAL Cluster 19 â†’ policija, leto, policist, napad, sodiÅ¡Äe, dejanje, Älovek, moÅ¡ki, otrok, kazniv, preiskava, kazniv dejanje, policijski, leten, kazen, drÅ¾ava, evro, zapor, primer, Å¾rtev
---- Vrhovno sodiÅ¡Äe zavrnilo JanÅ¡evo zahtevo za varstvo zakonitosti v primeru razÅ¾alitve novinark
---- Na HrvaÅ¡kem pridrÅ¾ali usluÅ¾benca zunanjega ministrstva zaradi suma tihotapljenja ljudi 
---- Å½rtev virtualne ugrabitve: kitajskega najstnika naÅ¡li v gozdovih Utaha
---- Britanska policija preiskuje prvo virtualno posilstvo v metaprostoru
---- V streljanju na Å¡oli v Iowi veÄ ranjenih
ðŸ§  OBLASTI Cluster 20 â†’ stranka, volitev, vlada, leto, predsednik, evropski, sodiÅ¡Äe, odstotek, nov, slovenija, drÅ¾ava, kandidat, komisija, zakon, poslanec, minister, svet, ministrstvo, parlament, volilen
---- Izraelsko vrhovno sodiÅ¡Äe razveljavilo kljuÄni del pravosodne reforme
---- Glasovanje ponavljajo Å¡e na osmih voliÅ¡Äih v Srbiji
---- Po zamenjavi oblasti na Poljskem tudi zamenjava predstavnika na beneÅ¡kem bienalu
---- V Srbiji po ponovljenem glasovanju na nekaterih voliÅ¡Äih potrdili zmago vladajoÄega SNS-ja
---- Golob v DZ Å¡e ni poslal predloga za kmetijskega ministra. Vojko AdamiÄ naj ne bi bil edini kandidat.
ðŸ§  ZGODOVINA Cluster 21 â†’ leto, muzej, slika, delo, nov, velik, papeÅ¾, del, evro, mesto, milijon, razstava, grad, svet, Älovek, vesoljski, Äas, star, stoletje, draÅ¾ba
---- PapeÅ¾ pri novoletni maÅ¡i poudaril pomen Å¾ensk za mir na svetu
---- Zgornji dom britanskega parlamenta lani kupil za veÄ kot sto tisoÄakov Å¡ampanjca
---- V nemÅ¡kih zbirkah Å¡e zmeraj najmanj 17.000 ÄloveÅ¡kih posmrtnih ostankov iz kolonialnega obdobja
---- Italijanski drÅ¾avni svet: Muzej J. Paul Getty lahko obdrÅ¾i sliko Jacopa Bassana
---- V bliÅ¾ini letaliÅ¡Äa v Veliki Britaniji izkopali srednjeveÅ¡ko pokopaliÅ¡Äe z nekaj ÄepeÄimi trupli
ðŸ§  UMETNOST Cluster 22 â†’ razstava, leto, delo, galerija, muzej, umetnost, umetnik, slovenski, fotografija, prostor, del, likoven, nov, projekt, kultura, kulturen, umetniÅ¡ki, svet, ljubljana, slovenija
---- Na razstavi v ZDA, ki razkriva drznost umetnikov vzhodnega bloka, tudi Vipotnik in Godina
---- Za vsakim muzejskim predmetom stoji restavrator z ogromno vedenja in iznajdljivosti
---- Narodna galerija v 2024: zbirka Kroples in secesijsko gibanje v Ukrajini
---- Nevidno in neplaÄano delo v umetnosti
---- V Univerzitetni knjiÅ¾nici Maribor o Å¾ivljenju in delu Antonije Å tupca
ðŸ§  GLASBA Cluster 23 â†’ leto, festival, album, koncert, glasba, glasben, pesem, nov, slovenski, skladba, skupina, dan, velik, zasedba, slovenija, mesto, svet, Älovek, dober, Äas
---- "Zasanjane melodije iz ÄeÅ¡kih gajev, ognjeviti ritmi ruske glasbe in vedre pesmi naÅ¡e deÅ¾ele"
---- Bogat program, bogat odziv: oddaja SreÄno 2024! poskrbela za nepozaben glasbeni vstop v novo leto
---- KatoliÅ¡ka cerkev zaznamuje svetovni dan miru
---- Novoletni koncert Dunajskih filharmonikov je tudi letos minil s slovensko udeleÅ¾bo
---- Duh iztekajoÄih se novoletnih praznikov
ðŸ§  LITERATURA Cluster 24 â†’ leto, knjiga, roman, nagrada, slovenski, zgodba, pesem, zbirka, Å¾ivljenje, Älovek, svet, delo, literaren, poezija, Äas, knjiÅ¾nica, jezik, pisatelj, beseda, nov
---- Ob izidu ArheoloÅ¡ke topografije Istre je druÅ¡tvo Histria podelilo tudi dvojico lovorik
---- Ustanova Velenjska knjiÅ¾na fundacija objavila literarna vabila za 23. Lirikonfest
---- Mark Twain
---- Sojenje Rushdiejevemu napadalcu odloÅ¾eno vsaj do izida pisateljeve knjige spominov
---- 280 naslovnic in 34 let oblikovanja "oken" v filozofske in psihoanalitiÄne knjige
ðŸ§  FILM Cluster 25 â†’ film, leto, predstava, filmski, festival, nagrada, slovenski, zgodba, dober, gledaliÅ¡Äe, nov, reÅ¾iser, serija, vloga, Älovek, Å¾ivljenje, svet, velik, Äas, Å¾enska
---- Vesolje med nami, slovenski mladinski film â€“ ponedeljek, 1. 1., ob 20.05 na TV SLO 1 
---- Skoraj stoletje kasneje MiÅ¡ek Miki in druga pomembna dela prehajajo v javno last
---- Pregled leta: s kakÅ¡nimi glasbenimi poslasticami je postreglo leto 2023?
---- Klub zadovoljnih Å¾ensk, ameriÅ¡ki film â€“ torek, 2. 1., ob 20.05 na TV SLO 1
---- VeÄ filmov preteklega Liffa v Cankarjevem domu, tudi v okviru abonmaja Liffe po Liffu


Initial embedding: Use SBERT embeddings (paraphrase-multilingual-mpnet-base-v2).
---
from sklearn.metrics import silhouette_score

silhouette_tf_idf = silhouette_score(umap_tf_idf, cluster_labels_tf_idf)
silhouette_sbert = silhouette_score(umap_sbert, cluster_labels_sbert)

from sklearn.metrics import davies_bouldin_score

db_index_tf_idf = davies_bouldin_score(umap_tf_idf, cluster_labels_tf_idf)
db_index_sbert = davies_bouldin_score(umap_sbert, cluster_labels_sbert)
!!!!
 Qualitative / Visual evaluation (key for interpretability):
Visual clarity of groups: Which embedding approach provides more intuitive, well-separated clusters on the UMAP 2D plot?
Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
Human-in-the-loop check:
Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.

1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
3. Evaluate and Compare:
Visual inspection: Check which method produces clearer, visually meaningful clusters.
Cluster evaluation metrics:
Silhouette Score: Measures how well separated clusters are.
Davies-Bouldin Index: Evaluates cluster compactness and separation.
Semantic/interpretability evaluation: Check how easy it is to interpret clusters.

----

To achieve explainability comparable to TF-IDF:

Use SBERT embeddings to cluster semantically meaningful groups.
Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.

-----
t-sne vs umap?
---------
also draw and compare extracted labels
---------
lematize?

---------
tune tfidf

-------
create clusters on embeddings not on umap projections



You're totally right to think this through carefully â€” if URL topic labels are meaningful, they should definitely group articles better than random. So getting an average NPMI of -0.0776 for URL topics might seem surprisingly low.

Letâ€™s dig deeper into why thatâ€™s happening, and how you can confirm that the labels are still better than random, even if clusters are better.

âœ… What could cause low NPMI on meaningful labels?

ðŸ“‰ 1. Small group sizes or highly uneven label distribution
NPMI depends on having enough co-occurrence signal.
If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
âœ… Fix:

Skip small groups:

if len(group_texts) < 5:
    continue
ðŸŒ€ 2. High lexical variety within URL topics
Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

Cover broad subtopics (e.g. politics, labor, society),
Use varied vocab (diluting co-occurrence),
Contain common words, reducing informativeness.
So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

ðŸ¤– 3. Tokenization, stopword handling, and preprocessing
If you're using raw word counts with minimal preprocessing:

NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).
âœ… Fixes:

Use lemmatization (not just tokenization).
Apply custom Slovene stopword list.
Lowercase, remove punctuation, etc.
ðŸ§ª 4. Random still possible to beat bad labels
If the label groups are too noisy or small, then random samples of size ~5 may coincidentally have more co-occurring words (especially common ones).

âœ… Test it: Compare to true random grouping

Letâ€™s prove it â€” create random groups the same size as your URL topic groups and compare NPMI.

âœ¨ Script Snippet:
def generate_random_groups(n_items, group_sizes):
    indices = np.random.permutation(n_items)
    groups = {}
    start = 0
    for i, size in enumerate(group_sizes):
        groups[f"rand_{i}"] = list(indices[start:start+size])
        start += size
    return groups

# Get URL label group sizes
label_group_sizes = [len(v) for v in label_groups.values() if len(v) >= 5]

# Generate random groups with same sizes
random_groups = generate_random_groups(len(texts), label_group_sizes)

# Compute NPMI for random groups
npmi_random = compute_npmi(random_groups, texts, stopwords=slovene_stopwords)

# Print comparison
print(f"Avg NPMI - Random: {np.mean(list(npmi_random.values())):.4f}")
print(f"Avg NPMI - URL Topics: {np.mean(list(npmi_labels.values())):.4f}")
ðŸ” Outcome:

If URL topics still beat random â†’ âœ… Youâ€™re right.
If random beats URL labels â†’ something's wrong with labels, text quality, or preprocessing.
âœ… Summary

Observation	Possible Cause	Fix/Check
URL labels give low NPMI	Small groups, varied content, weak keyword overlap	Filter small groups, lemmatize, clean
Clusters beat URL labels	HDBSCAN found tighter semantic groups	This is OK â€” it shows your pipeline adds value
Random matches URL NPMI	Label noise, sparse keywords	Run comparison with controlled random groups
Would you like me to wrap this random vs. label NPMI test into a reusable function and print all three side-by-side?






Maybe recursive clustering

WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
compare to
umap to 2D -> hbdscan

Test integrity when using random, cluster and knn cluster corr

preprocess for without adjectives








ntic alignment using embeddings, and it can give you a more direct, model-based evaluation of clustering quality â€” especially when traditional clustering metrics fail (like silhouette score on 2D projections).

âœ… Why Use Embedding-Based Evaluation?
Traditional metrics (e.g., Silhouette, Davies-Bouldin) rely on distances between points and fail when:

Clusters are non-spherical
Data is high-dimensional
There's semantic overlap between labels
Using embeddings allows you to evaluate semantic coherence and separation independently of geometry.

âœ… Options for Embedding-Based Evaluation
1. Intra-cluster vs Inter-cluster Cosine Similarity

Measure how similar articles in a cluster are to each other (intra)
Compare to similarity between articles in different clusters (inter)
The higher the gap, the better
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def average_intra_inter_similarity(embeddings, labels):
    unique_clusters = np.unique(labels[labels >= 0])
    intra_sims = []
    inter_sims = []

    for cluster in unique_clusters:
        idx = np.where(labels == cluster)[0]
        cluster_embeds = embeddings[idx]
        if len(cluster_embeds) < 2:
            continue

        sim_matrix = cosine_similarity(cluster_embeds)
        intra_sims.append(np.mean(sim_matrix[np.triu_indices_from(sim_matrix, k=1)]))

        # Compare to a few random other clusters
        other_idx = np.where(labels != cluster)[0]
        sampled_other = np.random.choice(other_idx, size=min(len(idx), len(other_idx)), replace=False)
        inter_sim = cosine_similarity(cluster_embeds, embeddings[sampled_other])
        inter_sims.append(np.mean(inter_sim))

    return np.mean(intra_sims), np.mean(inter_sims)
2. Cluster Centroid Cosine Distances

Compute each cluster's centroid
Measure:
How close points are to their centroid (cohesion)
How far centroids are from each other (separation)
âœ… Why It Might Be Better
Works directly on SBERT or BERT embeddings (semantic!)
Not distorted by dimensionality reduction (e.g., UMAP or t-SNE)
Unsupervised â€” you don't need ground-truth labels
Doesnâ€™t assume spherical geometry like Silhouette
âœ… Alternative: NMI or ARI with Ground Truth Labels
If you have something like URL-based topics, you can also compute:

from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

nmi = normalized_mutual_info_score(true_labels, predicted_clusters)
ari = adjusted_rand_score(true_labels, predicted_clusters)
But embeddings give you a more model-based semantic check.

Would you like me to help implement this for your current SBERT setup?

You said:
i think that you once suggested the word coocurrance or something like that to measure semantics. How does that compare
ChatGPT said:
Yes, youâ€™re right â€” earlier I suggested using word co-occurrence-based metrics, like PMI/NPMI, to evaluate the semantic coherence of clusters. Letâ€™s compare the two approaches in depth:

âš–ï¸ Comparison: Word Co-occurrence (PMI/NPMI) vs Embedding Similarity
Aspect	PMI/NPMI (Co-occurrence)	Embedding-based (SBERT etc.)
Goal	Measure if words in cluster co-occur meaningfully	Measure if article meanings are semantically close
Unit	Words	Articles (vectors)
Basis	Word frequency & co-occurrence	Language model embeddings
Type	Symbolic/statistical	Neural/semantic
Pros	- Unsupervised
- Interpretable (keyword-level)
- Captures topical coherence	- Captures deep semantic relationships
- Works well for short texts
- Does not depend on token overlap
Cons	- Sensitive to corpus size
- Doesnâ€™t handle synonyms well
- Ignores meaning beyond word co-occurrence	- Opaque (harder to interpret)
- Can fail for mixed-topic clusters
Output	Coherence scores per cluster (e.g. NPMI)	Similarity stats per cluster (intra/inter)
âœ… When to Use Each
Use PMI/NPMI if:

You care about word-level coherence
You want to interpret what a cluster is about
Youâ€™re working with many short docs (titles, leads)
Use Embedding-based similarity if:

You want to test semantic tightness
Your documents are longer or richer (e.g., paragraphs)
Youâ€™re clustering based on embeddings (SBERT etc.)
ðŸ§  Best Practice: Use Both
Many NLP papers evaluate topic clusters using both:

NPMI: For keyword coherence
Cosine similarity: For semantic tightness in embedding space
Together, they give a well-rounded view:

Do words co-occur?
Are texts semantically similar?
Do generated cluster labels reflect the content?
Would you like a script that outputs both NPMI and embedding coherence for your clusters?

You said:
so can you use pmi/npmi to extract meaningful words for cluster?
ChatGPT said:
Yes â€” you can absolutely use PMI/NPMI to extract meaningful keywords for a cluster. In fact, this is one of the original purposes of those metrics in topic modeling!

âœ… How PMI/NPMI Helps for Keyword Extraction
Instead of just picking top TF-IDF words, you rank combinations of words that co-occur more than expected. That way, you get:

Thematically consistent words
Fewer irrelevant or generic words (like â€œdejalaâ€ or â€œlahkoâ€)
Words that tend to appear together in the same context
âœ… Example Use: Gensimâ€™s CoherenceModel
You can do something like:

from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

def extract_coherent_keywords(token_lists, top_n=10):
    dictionary = Dictionary(token_lists)
    corpus = [dictionary.doc2bow(text) for text in token_lists]

    cm = CoherenceModel(
        texts=token_lists,
        corpus=corpus,
        dictionary=dictionary,
        coherence='c_npmi',
        topn=top_n
    )

    # This will rank words by their NPMI-based coherence
    return cm.get_coherence_per_topic()
This is typically used after clustering documents, where each cluster is treated like a "topic".

âœ… Compared to TF-IDF
Feature	TF-IDF	PMI/NPMI
Focus	Individual word frequency	Word pairs and co-occurrence
Pros	Simple, fast	Captures topical coherence
Common artifacts	Words like â€œdejalaâ€, â€œletoâ€, etc.	Fewer generic terms
Interpretability	Medium	High (shows word relationships)
Language dependent?	Somewhat	Less so â€” works on token co-occurrence
ðŸ§  Best Strategy
Use Stanza-preprocessed token lists (e.g. NOUN, PROPN, ADJ only)
For each cluster, treat the collection of docs as a â€œtopicâ€
Run CoherenceModel with coherence='c_npmi' to extract top words
Would you like a full working example that does this per cluster?

You said:
for explainability is there any reason other than speed, to use methods other than llm or bert embeddings and summarization for clusters, if i have whole articles embedded and can also embed only leads if i would need to to reduce the load
ChatGPT said:
Thatâ€™s a great and forward-thinking question â€” and the short answer is:

If you're already using LLMs or BERT-style embeddings + summarization, youâ€™re near the best you can do in terms of explainability â€” especially if you can afford the compute.
But here's the nuanced breakdown:
âœ… When LLMs/BERT Embeddings & Summarization Win

They are ideal when you want:

Semantic depth: They capture context, tone, and meaning
Natural summaries: You can generate human-readable cluster labels
Consistency: They behave similarly across documents
Multilingual support: Especially useful for Slovene
Explainability via sentence: Not just top keywords, but a sentence that can describe a whole group
So when might other methods still make sense?
Case	Alternative Might Be Better?	Why
Compute-limited environments	Yes	TF-IDF is fast & light
Need transparent scoring	Yes	TF-IDF & NPMI are explicitly ranked
Keyword interpretability is preferred	Sometimes	You might want to show top-N words, not just a sentence
Auditability / reproducibility	Yes	Easier to explain to non-technical users
Short documents (titles only)	Maybe	Keywords might perform just as well as LLMs
Example: Combine the Best of Both
A very explainable pipeline might look like:

Cluster documents (with SBERT or similar)
Extract top NPMI keywords per cluster
Generate sentence summaries using an LLM on those keywords
Optionally: Add a few representative article titles to each cluster
Display all in a UI: sentence + keywords + example headlines
TL;DR
If:

You can run mBART or Flan-T5
You have document-level embeddings
Youâ€™re using keyword-based summaries only as a fallback
Then youâ€™re already in the best explainability zone â€” and donâ€™t need simpler alternatives unless for speed, audience clarity, or audits.

Let me know if you'd like to generate cluster cards or summaries that combine:

Sentence label
Top keywords
Most representative article title(s) (e.g., closest to cluster centroid)

also how well description sentences are suited

























Ukraine has to help Slovenia and Ukraine has to help Russia

Ilka Stuhec enters fifth best time at world's first world in royal discipline

The government has confirmed the source of the legislation on the ownership of workers .

Los Angeles Kings win series

Slovenia has banned the transport of dead and displaced animals

Divaa is a city in the southwestern part of the country .

Dallas' coaches played the 45th series of finals in the league of basketball , and until now only two games were eliminated from 4:0 .

In the car, we made the first mile with a new damage to the tire, which is technically similar to the superpower, so this time we looked a little more detailed about what the new one brings. It's gonna be a lot of years before electric vehicles are blown off the roads with internal combustion engines. At Volvo, only 100% of electric vehicles were thought to be manufactured. The executive director of Volva Jim Rowan was moved to change.

electric cars are a big part of the electric car industry .

The Four concert will be broadcast live on Slovenian TV on 23 September at 20 a.m.



{0: " HOKEJ",
1: " KOÅ ARKA",
2: " AMERIKA - VLADA",
3: " DIRKANJE - TUDI KOLESARJENJE",
4: " TENIS",
5: " IZRAEL, PALESTINA",
6: " SMUÄŒARSKI SKOKI - TUDI OLIMPIJSKE DISCIPLINE",
7: " NOGOMET",
8: " SLOVENSKI Å PORTI - TUDI KOÅ ARKA IN HOKEJ",
9: " SLOVENSKI Å PORTI - TUDI KOÅ ARKA IN HOKEJ",
10: " AVTOMOBILI, LETALA",
11: " GOSPODARSTVO",
12: " GASILSTVO, POÅ½ARI",
13: " TURIZEM",
14: " VREME - TUDI TURIZEM",
15: " SLOVENSKA POLITIKA",
16: " NESREÄŒE",
17: " POMEMBNI LJUDJE",
18: " ZAKONODAJA - TUDI GOSPODARSTVO",
19: " SODIÅ ÄŒE, POLICIJA, KRIMINAL",
20: " OBLASTI",
21: " ZGODOVINA",
22: " UMETNOST",
23: " GLASBA",
24: " LITERATURA",
25: " FILM"}