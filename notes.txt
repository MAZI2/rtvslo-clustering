Initial embedding: Use SBERT embeddings (paraphrase-multilingual-mpnet-base-v2).
---
from sklearn.metrics import silhouette_score

silhouette_tf_idf = silhouette_score(umap_tf_idf, cluster_labels_tf_idf)
silhouette_sbert = silhouette_score(umap_sbert, cluster_labels_sbert)

from sklearn.metrics import davies_bouldin_score

db_index_tf_idf = davies_bouldin_score(umap_tf_idf, cluster_labels_tf_idf)
db_index_sbert = davies_bouldin_score(umap_sbert, cluster_labels_sbert)
!!!!
 Qualitative / Visual evaluation (key for interpretability):
Visual clarity of groups: Which embedding approach provides more intuitive, well-separated clusters on the UMAP 2D plot?
Interpretability: After clustering, inspect top terms (via TF-IDF within clusters) to qualitatively evaluate if clusters make sense thematically or semantically.
Human-in-the-loop check:
Show random samples from clusters and verify if similar articles are indeed clustered together. Evaluate the quality of cluster labels or keywords extracted from each embedding.

1. Compute Embeddings:
TF-IDF embeddings â†’ Sparse, high-dimensional vectors.
SBERT embeddings â†’ Dense, semantic vectors.
2. Project Both Using UMAP:
Use identical UMAP parameters for both embeddings (e.g., n_neighbors=15, min_dist=0.1, metric='cosine' for consistency).
3. Evaluate and Compare:
Visual inspection: Check which method produces clearer, visually meaningful clusters.
Cluster evaluation metrics:
Silhouette Score: Measures how well separated clusters are.
Davies-Bouldin Index: Evaluates cluster compactness and separation.
Semantic/interpretability evaluation: Check how easy it is to interpret clusters.

----

To achieve explainability comparable to TF-IDF:

Use SBERT embeddings to cluster semantically meaningful groups.
Then apply TF-IDF or KeyBERT keyword extraction to each cluster independently, gaining TF-IDF-like interpretability at the end.

-----
t-sne vs umap?
---------
also draw and compare extracted labels
---------
lematize?

---------
tune tfidf

-------
create clusters on embeddings not on umap projections



You're totally right to think this through carefully â€” if URL topic labels are meaningful, they should definitely group articles better than random. So getting an average NPMI of -0.0776 for URL topics might seem surprisingly low.

Letâ€™s dig deeper into why thatâ€™s happening, and how you can confirm that the labels are still better than random, even if clusters are better.

âœ… What could cause low NPMI on meaningful labels?

ðŸ“‰ 1. Small group sizes or highly uneven label distribution
NPMI depends on having enough co-occurrence signal.
If some labels only have 2â€“3 articles, the keywords are too sparse â†’ unstable or negative NPMI.
âœ… Fix:

Skip small groups:

if len(group_texts) < 5:
    continue
ðŸŒ€ 2. High lexical variety within URL topics
Even if topics like "slovenija/preberite-tudi" are meaningful, the articles may:

Cover broad subtopics (e.g. politics, labor, society),
Use varied vocab (diluting co-occurrence),
Contain common words, reducing informativeness.
So although URL labels group by editorial category, they may not be lexically tight enough to produce good NPMI.

ðŸ¤– 3. Tokenization, stopword handling, and preprocessing
If you're using raw word counts with minimal preprocessing:

NPMI might reflect co-occurrence of non-thematic words (like verbs, connectors).
Or you may have low overlap between documents because of inconsistent text structure (short leads, etc).
âœ… Fixes:

Use lemmatization (not just tokenization).
Apply custom Slovene stopword list.
Lowercase, remove punctuation, etc.
ðŸ§ª 4. Random still possible to beat bad labels
If the label groups are too noisy or small, then random samples of size ~5 may coincidentally have more co-occurring words (especially common ones).

âœ… Test it: Compare to true random grouping

Letâ€™s prove it â€” create random groups the same size as your URL topic groups and compare NPMI.

âœ¨ Script Snippet:
def generate_random_groups(n_items, group_sizes):
    indices = np.random.permutation(n_items)
    groups = {}
    start = 0
    for i, size in enumerate(group_sizes):
        groups[f"rand_{i}"] = list(indices[start:start+size])
        start += size
    return groups

# Get URL label group sizes
label_group_sizes = [len(v) for v in label_groups.values() if len(v) >= 5]

# Generate random groups with same sizes
random_groups = generate_random_groups(len(texts), label_group_sizes)

# Compute NPMI for random groups
npmi_random = compute_npmi(random_groups, texts, stopwords=slovene_stopwords)

# Print comparison
print(f"Avg NPMI - Random: {np.mean(list(npmi_random.values())):.4f}")
print(f"Avg NPMI - URL Topics: {np.mean(list(npmi_labels.values())):.4f}")
ðŸ” Outcome:

If URL topics still beat random â†’ âœ… Youâ€™re right.
If random beats URL labels â†’ something's wrong with labels, text quality, or preprocessing.
âœ… Summary

Observation	Possible Cause	Fix/Check
URL labels give low NPMI	Small groups, varied content, weak keyword overlap	Filter small groups, lemmatize, clean
Clusters beat URL labels	HDBSCAN found tighter semantic groups	This is OK â€” it shows your pipeline adds value
Random matches URL NPMI	Label noise, sparse keywords	Run comparison with controlled random groups
Would you like me to wrap this random vs. label NPMI test into a reusable function and print all three side-by-side?






Maybe recursive clustering

WORKS umap to 50D -> noralize(cosine dist) -> hbdscan 
compare to
umap to 2D -> hbdscan

Test integrity when using random, cluster and knn cluster corr

preprocess for without adjectives








ntic alignment using embeddings, and it can give you a more direct, model-based evaluation of clustering quality â€” especially when traditional clustering metrics fail (like silhouette score on 2D projections).

âœ… Why Use Embedding-Based Evaluation?
Traditional metrics (e.g., Silhouette, Davies-Bouldin) rely on distances between points and fail when:

Clusters are non-spherical
Data is high-dimensional
There's semantic overlap between labels
Using embeddings allows you to evaluate semantic coherence and separation independently of geometry.

âœ… Options for Embedding-Based Evaluation
1. Intra-cluster vs Inter-cluster Cosine Similarity

Measure how similar articles in a cluster are to each other (intra)
Compare to similarity between articles in different clusters (inter)
The higher the gap, the better
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def average_intra_inter_similarity(embeddings, labels):
    unique_clusters = np.unique(labels[labels >= 0])
    intra_sims = []
    inter_sims = []

    for cluster in unique_clusters:
        idx = np.where(labels == cluster)[0]
        cluster_embeds = embeddings[idx]
        if len(cluster_embeds) < 2:
            continue

        sim_matrix = cosine_similarity(cluster_embeds)
        intra_sims.append(np.mean(sim_matrix[np.triu_indices_from(sim_matrix, k=1)]))

        # Compare to a few random other clusters
        other_idx = np.where(labels != cluster)[0]
        sampled_other = np.random.choice(other_idx, size=min(len(idx), len(other_idx)), replace=False)
        inter_sim = cosine_similarity(cluster_embeds, embeddings[sampled_other])
        inter_sims.append(np.mean(inter_sim))

    return np.mean(intra_sims), np.mean(inter_sims)
2. Cluster Centroid Cosine Distances

Compute each cluster's centroid
Measure:
How close points are to their centroid (cohesion)
How far centroids are from each other (separation)
âœ… Why It Might Be Better
Works directly on SBERT or BERT embeddings (semantic!)
Not distorted by dimensionality reduction (e.g., UMAP or t-SNE)
Unsupervised â€” you don't need ground-truth labels
Doesnâ€™t assume spherical geometry like Silhouette
âœ… Alternative: NMI or ARI with Ground Truth Labels
If you have something like URL-based topics, you can also compute:

from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score

nmi = normalized_mutual_info_score(true_labels, predicted_clusters)
ari = adjusted_rand_score(true_labels, predicted_clusters)
But embeddings give you a more model-based semantic check.

Would you like me to help implement this for your current SBERT setup?

You said:
i think that you once suggested the word coocurrance or something like that to measure semantics. How does that compare
ChatGPT said:
Yes, youâ€™re right â€” earlier I suggested using word co-occurrence-based metrics, like PMI/NPMI, to evaluate the semantic coherence of clusters. Letâ€™s compare the two approaches in depth:

âš–ï¸ Comparison: Word Co-occurrence (PMI/NPMI) vs Embedding Similarity
Aspect	PMI/NPMI (Co-occurrence)	Embedding-based (SBERT etc.)
Goal	Measure if words in cluster co-occur meaningfully	Measure if article meanings are semantically close
Unit	Words	Articles (vectors)
Basis	Word frequency & co-occurrence	Language model embeddings
Type	Symbolic/statistical	Neural/semantic
Pros	- Unsupervised
- Interpretable (keyword-level)
- Captures topical coherence	- Captures deep semantic relationships
- Works well for short texts
- Does not depend on token overlap
Cons	- Sensitive to corpus size
- Doesnâ€™t handle synonyms well
- Ignores meaning beyond word co-occurrence	- Opaque (harder to interpret)
- Can fail for mixed-topic clusters
Output	Coherence scores per cluster (e.g. NPMI)	Similarity stats per cluster (intra/inter)
âœ… When to Use Each
Use PMI/NPMI if:

You care about word-level coherence
You want to interpret what a cluster is about
Youâ€™re working with many short docs (titles, leads)
Use Embedding-based similarity if:

You want to test semantic tightness
Your documents are longer or richer (e.g., paragraphs)
Youâ€™re clustering based on embeddings (SBERT etc.)
ðŸ§  Best Practice: Use Both
Many NLP papers evaluate topic clusters using both:

NPMI: For keyword coherence
Cosine similarity: For semantic tightness in embedding space
Together, they give a well-rounded view:

Do words co-occur?
Are texts semantically similar?
Do generated cluster labels reflect the content?
Would you like a script that outputs both NPMI and embedding coherence for your clusters?

You said:
so can you use pmi/npmi to extract meaningful words for cluster?
ChatGPT said:
Yes â€” you can absolutely use PMI/NPMI to extract meaningful keywords for a cluster. In fact, this is one of the original purposes of those metrics in topic modeling!

âœ… How PMI/NPMI Helps for Keyword Extraction
Instead of just picking top TF-IDF words, you rank combinations of words that co-occur more than expected. That way, you get:

Thematically consistent words
Fewer irrelevant or generic words (like â€œdejalaâ€ or â€œlahkoâ€)
Words that tend to appear together in the same context
âœ… Example Use: Gensimâ€™s CoherenceModel
You can do something like:

from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

def extract_coherent_keywords(token_lists, top_n=10):
    dictionary = Dictionary(token_lists)
    corpus = [dictionary.doc2bow(text) for text in token_lists]

    cm = CoherenceModel(
        texts=token_lists,
        corpus=corpus,
        dictionary=dictionary,
        coherence='c_npmi',
        topn=top_n
    )

    # This will rank words by their NPMI-based coherence
    return cm.get_coherence_per_topic()
This is typically used after clustering documents, where each cluster is treated like a "topic".

âœ… Compared to TF-IDF
Feature	TF-IDF	PMI/NPMI
Focus	Individual word frequency	Word pairs and co-occurrence
Pros	Simple, fast	Captures topical coherence
Common artifacts	Words like â€œdejalaâ€, â€œletoâ€, etc.	Fewer generic terms
Interpretability	Medium	High (shows word relationships)
Language dependent?	Somewhat	Less so â€” works on token co-occurrence
ðŸ§  Best Strategy
Use Stanza-preprocessed token lists (e.g. NOUN, PROPN, ADJ only)
For each cluster, treat the collection of docs as a â€œtopicâ€
Run CoherenceModel with coherence='c_npmi' to extract top words
Would you like a full working example that does this per cluster?

You said:
for explainability is there any reason other than speed, to use methods other than llm or bert embeddings and summarization for clusters, if i have whole articles embedded and can also embed only leads if i would need to to reduce the load
ChatGPT said:
Thatâ€™s a great and forward-thinking question â€” and the short answer is:

If you're already using LLMs or BERT-style embeddings + summarization, youâ€™re near the best you can do in terms of explainability â€” especially if you can afford the compute.
But here's the nuanced breakdown:
âœ… When LLMs/BERT Embeddings & Summarization Win

They are ideal when you want:

Semantic depth: They capture context, tone, and meaning
Natural summaries: You can generate human-readable cluster labels
Consistency: They behave similarly across documents
Multilingual support: Especially useful for Slovene
Explainability via sentence: Not just top keywords, but a sentence that can describe a whole group
So when might other methods still make sense?
Case	Alternative Might Be Better?	Why
Compute-limited environments	Yes	TF-IDF is fast & light
Need transparent scoring	Yes	TF-IDF & NPMI are explicitly ranked
Keyword interpretability is preferred	Sometimes	You might want to show top-N words, not just a sentence
Auditability / reproducibility	Yes	Easier to explain to non-technical users
Short documents (titles only)	Maybe	Keywords might perform just as well as LLMs
Example: Combine the Best of Both
A very explainable pipeline might look like:

Cluster documents (with SBERT or similar)
Extract top NPMI keywords per cluster
Generate sentence summaries using an LLM on those keywords
Optionally: Add a few representative article titles to each cluster
Display all in a UI: sentence + keywords + example headlines
TL;DR
If:

You can run mBART or Flan-T5
You have document-level embeddings
Youâ€™re using keyword-based summaries only as a fallback
Then youâ€™re already in the best explainability zone â€” and donâ€™t need simpler alternatives unless for speed, audience clarity, or audits.

Let me know if you'd like to generate cluster cards or summaries that combine:

Sentence label
Top keywords
Most representative article title(s) (e.g., closest to cluster centroid)